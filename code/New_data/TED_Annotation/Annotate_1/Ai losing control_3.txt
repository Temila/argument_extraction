Death by science fiction, on the other hand, is fun, and one of the things that worries me most about the development of AI at this point is that we seem unable to marshal an appropriate emotional response to the dangers that lie ahead.

Almost by definition, this is the worst thing that's ever happened in human history.

But that isn't the most likely scenario.

The concern is really that we will build machines that are so much more competent than we are that the slightest divergence between their goals and our own could destroy us.

The concern is that we will one day build machines that, whether they're conscious or not, could treat us with similar disregard.

Intelligence is a matter of information processing in physical systems.

It's crucial to realize that the rate of progress doesn't matter, because any progress is enough to get us into the end zone.

It seems overwhelmingly likely, however, that the spectrum of intelligence extends much further than we currently conceive, and if we build machines that are more intelligent than we are, they will very likely explore this spectrum in ways that we can't imagine, and exceed us in ways that we can't imagine.

And then we risk what the mathematician IJ Good called an "intelligence explosion," that the process could get away from us.

Well, electronic circuits function about a million times faster than biochemical ones, so this machine should think about a million times faster than the minds that built it

Well, this machine would be the perfect labor-saving device.

One researcher has said, "Worrying about AI safety is like worrying about overpopulation on Mars."

Absent a willingness to immediately put this new wealth to the service of all humanity, a few trillionaires could grace the covers of our business magazines while the rest of the world would be free to starve.
