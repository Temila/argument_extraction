Death by science fiction, on the other hand, is fun, and one of the things that worries me most about the development of AI at this point is that we seem unable to marshal an appropriate emotional response to the dangers that lie ahead.

It's not that our machines will become spontaneously malevolent.

The concern is really that we will build machines that are so much more competent than we are that the slightest divergence between their goals and our own could destroy us.

The concern is that we will one day build machines that, whether they're conscious or not, could treat us with similar disregard.

I bet there are those of you who doubt that superintelligent AI is possible, much less inevitable.

Intelligence is a matter of information processing in physical systems.

I mean, there's just atoms in here, and as long as we continue to build systems of atoms that display more and more intelligent behavior, we will eventually, unless we are interrupted, we will eventually build general intelligence into our machines.

And then we risk what the mathematician IJ Good called an "intelligence explosion," that the process could get away from us.

And given the value of intelligence -- I mean, intelligence is either the source of everything we value or we need it to safeguard everything we value.

Well, electronic circuits function about a million times faster than biochemical ones, so this machine should think about a million times faster than the minds that built it.

The deeper problem is that building superintelligent AI on its own seems likely to be easier than building superintelligent AI and having the completed neuroscience that allows us to seamlessly integrate our minds with it.

One researcher has said, "Worrying about AI safety is like worrying about overpopulation on Mars."

But the moment we admit that information processing is the source of intelligence, that some appropriate computational system is what the basis of intelligence is, and we admit that we will improve these systems continuously, and we admit that the horizon of cognition very likely far exceeds what we currently know, then we have to admit that we are in the process of building some sort of god.