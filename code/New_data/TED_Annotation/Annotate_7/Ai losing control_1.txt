It's really a failure to detect a certain kind of danger.

And in fact, I think it's very difficult to see how they won't destroy us or inspire us to destroy ourselves.

Now, this is often caricatured, as I have here, as a fear that armies of malicious robots will attack us.

And then we risk what the mathematician IJ Good called an "intelligence explosion," that the process could get away from us.

Intelligence is a matter of information processing in physical systems.

The second assumption is that we will keep going.

One researcher has said, "Worrying about AI safety is like worrying about overpopulation on Mars."

Finally, we don't stand on a peak of intelligence, or anywhere near it, likely.

(Laughter)  It seems overwhelmingly likely, however, that the spectrum of intelligence extends much further than we currently conceive, and if we build machines that are more intelligent than we are, they will very likely explore this spectrum in ways that we can't imagine, and exceed us in ways that we can't imagine.